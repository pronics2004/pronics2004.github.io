---
layout: default
---

# Prasanna Sattigeri

<img src="profile.png" alt="profile" width="180"/>

I am a **Principal Research Scientist** at [IBM Research AI](https://research.ibm.com/) and the [MIT-IBM Watson AI Lab](https://mitibmwatsonailab.mit.edu/), where my primary focus is on developing reliable AI solutions.

My current projects establish both theoretical frameworks and practical systems that ensure large language models are reliable and trustworthy. I lead the [Granite Guardian](https://github.com/ibm-granite/granite-guardian) project — IBM's state-of-the-art LLM safeguarding models.

### Research Interests

- **Generative Modeling** and Large Language Models
- **Uncertainty Quantification** for AI systems
- **Learning with Limited Data**
- **LLM Governance, Safety, and Alignment**
- **Human-AI Collaboration**
- **Agentic AI Systems**

### Open-Source Contributions

I lead and contribute to widely-adopted trustworthy AI toolkits:

- [Granite Guardian](https://github.com/ibm-granite/granite-guardian) — LLM safeguarding for risks, jailbreaking, and hallucination detection (#1 on [GuardBench](https://research.ibm.com/blog/granite-guardian-tops-guardbench))
- [AI Fairness 360](https://github.com/Trusted-AI/AIF360) — Detecting and mitigating bias in ML models (2,300+ GitHub stars)
- [AI Explainability 360](https://github.com/Trusted-AI/AIX360) — Explaining AI decisions
- [Uncertainty Quantification 360](https://github.com/IBM/UQ360) — Quantifying uncertainty in AI predictions
- [ICX360](https://github.com/IBM/ICX360) — Multi-level explanations for generative language models

---

## Links

- [Google Scholar](https://scholar.google.com/citations?hl=en&user=m-s38ikAAAAJ&view_op=list_works) — 7,700+ citations
- [IBM Research Profile](https://research.ibm.com/people/prasanna-sattigeri)
- [MIT-IBM Watson AI Lab](https://mitibmwatsonailab.mit.edu/people/prasanna-sattigeri/)
- [Papers With Code](https://paperswithcode.com/author/prasanna-sattigeri)
- [Detailed CV](CV.html)

---

## Recent News

### 2025

- **April 2025** — IBM Research Blog: [Granite Guardian tops third-party AI benchmark](https://research.ibm.com/blog/granite-guardian-tops-guardbench) — Granite Guardian holds 6 of top 10 spots on GuardBench, scoring 86% across 40 datasets
- **April 2025** — Paper accepted at **ACL 2025**: ["Multi-Level Explanations for Generative Language Models"](https://aclanthology.org/2025.acl-long.1553/)
- **April 2025** — Paper accepted at **NAACL 2025**: ["Evaluating the Prompt Steerability of Large Language Models"](https://aclanthology.org/2025.naacl-long.400/)
- **April 2025** — Paper accepted at **NAACL 2025 Industry Track**: ["Granite Guardian: Comprehensive LLM Safeguarding"](https://aclanthology.org/2025.naacl-industry.49/)
- **March 2025** — IBM Research Blog: [IBM Granite now has adapters designed to control AI outputs](https://research.ibm.com/blog/Granite-adapter-experiments) — LLM calibration work from MIT-IBM Watson AI Lab
- **February 2025** — New preprint: ["On the Trustworthiness of Generative Foundation Models"](https://arxiv.org/abs/2502.14296) — comprehensive guideline and assessment (66 co-authors)
- **February 2025** — New preprint: ["Agentic AI Needs a Systems Theory"](https://arxiv.org/abs/2503.00237) — position paper on holistic approaches to agentic AI
- **February 2025** — IBM Research Blog: [How we slimmed down Granite Guardian](https://research.ibm.com/blog/ibm-granite-guardian-5b-3b) — Granite Guardian 3.2 5B and MoE 3B models

### 2024

- **December 2024** — Released [Granite Guardian](https://arxiv.org/abs/2412.07724) — achieving AUC 0.871 on harmful content and 0.854 on RAG-hallucination benchmarks
- **October 2024** — New preprint: ["Building a Foundational Guardrail for General Agentic Systems"](https://arxiv.org/abs/2510.09781) — safeguarding agentic AI via synthetic data
- **October 2024** — New preprint: ["Graph-based Uncertainty Metrics for Long-form LLM Outputs"](https://arxiv.org/abs/2410.20783)
- **December 2024** — Papers accepted at **NeurIPS 2024**:
  - "Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?"
  - "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts"
  - "Attack Atlas: Challenges and Pitfalls in Red Teaming GenAI"
- **November 2024** — Papers accepted at **EMNLP 2024**:
  - ["Language Models in Dialogue: Conversational Maxims for Human-AI Interactions"](https://aclanthology.org/2024.findings-emnlp.843/)
  - "Value Alignment from Unstructured Text" (Industry Track)
- **July 2024** — Paper accepted at **ICML 2024**: "Thermometer: Towards Universal Calibration for Large Language Models"
- **June 2024** — Invited talk on LLM Governance and Alignment at the NAACL TrustNLP Workshop. [Slides](slides/naacl_trustnlp_talk.pdf)
- **2024** — Panel and talk on Reliable AI-assisted Decision Making at the National Academy of Sciences Decadal Survey
- **2024** — Speaking at MIT AI Conference on AI ethics and change management

### 2023

- **December 2023** — Papers accepted at **NeurIPS 2023**:
  - "Efficient Equivariant Transfer Learning from Pretrained Models"
  - ["Effective Human-AI Teams via Learned Natural Language Rules and Onboarding"](https://github.com/clinicalml/onboarding_human_ai)
- **August 2023** — Invited talk on Uncertainty Calibration at KDD Workshop on Uncertainty Reasoning
- **August 2023** — Panel on Generative AI and Safety at DSHealth Workshop, KDD
- **August 2023** — Panel on Trustworthy LLMs at AI for Open Society Day, KDD
- **February 2023** — Papers at **AAAI 2023** and **EACL 2023**

---

## Featured Research

### Granite Guardian — State-of-the-Art LLM Safeguarding

I lead the [Granite Guardian](https://github.com/ibm-granite/granite-guardian) project at IBM Research, developing open-source models for LLM risk detection:

- **#1 on GuardBench** — First independent AI guardrail benchmark (86% accuracy across 40 datasets)
- **#1 on REVEAL** — Reasoning chain correctness evaluation (outperforms GPT-4o)
- **#3 on LLM-AggreFact** — Comprehensive fact-checking benchmark
- Covers social bias, profanity, violence, jailbreaking, and RAG hallucination risks
- Available on [Hugging Face](https://huggingface.co/ibm-granite) and [GitHub](https://github.com/ibm-granite/granite-guardian)

### MIT-IBM Watson AI Lab Collaborations

- **With Prof. Greg Wornell (MIT)**: Trustworthy Learning with Limited Data — uncertainty quantification and calibration for foundation models
- **With Prof. David Sontag (MIT)**: Human-Centric AI — algorithms for shared decision making and human-AI team onboarding

---

## Professional Service

- **Associate Editor**: [Pattern Recognition](https://www.journals.elsevier.com/pattern-recognition/editorial-board/prasanna-sattigeri) (Elsevier)
- **Senior Program Committee / Area Chair**: AAAI, ICLR, NeurIPS, ICML
- **Reviewer**: NeurIPS, ICML, AAAI, ICLR, EMNLP, ACL, IEEE TPAMI
