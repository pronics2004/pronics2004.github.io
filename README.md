# Prasanna Sattigeri

<img src="profile.png" alt="profile" width="200"/>

## About Me

I am a **Principal Research Scientist** at [IBM Research AI](https://research.ibm.com/) and the [MIT-IBM Watson AI Lab](https://mitibmwatsonailab.mit.edu/), where my primary focus is on developing reliable AI solutions.

### Research Interests

My research interests encompass:
- **Generative Modeling** and Large Language Models
- **Uncertainty Quantification** for AI systems
- **Learning with Limited Data**
- **LLM Governance, Safety, and Alignment**
- **Human-AI Collaboration**

My current projects are focused on establishing both theoretical frameworks and practical systems that ensure large language models are reliable and trustworthy.

### Open-Source Contributions

I have played a significant role in the development of several widely-adopted trustworthy AI toolkits:
- [AI Fairness 360](https://aif360.mybluemix.net/) - Detecting and mitigating bias in ML models
- [AI Explainability 360](https://aix360.mybluemix.net/) - Explaining AI decisions
- [Uncertainty Quantification 360](https://uq360.mybluemix.net/) - Quantifying uncertainty in AI predictions
- [Granite Guardian](https://huggingface.co/ibm-granite) - LLM safeguarding for harmful content, jailbreaking, and hallucination detection

---

## Links

- [Google Scholar](https://scholar.google.com/citations?hl=en&user=m-s38ikAAAAJ&view_op=list_works)
- [IBM Research Profile](https://research.ibm.com/people/prasanna-sattigeri)
- [Detailed CV](CV.md)

---

## Recent News

### 2025

- **March 2025** - IBM Research Blog: [IBM Granite now has adapters designed to control AI outputs](https://research.ibm.com/blog/Granite-adapter-experiments) - featuring our work on LLM calibration from MIT-IBM Watson AI Lab
- **February 2025** - IBM Research Blog: [How we slimmed down Granite Guardian](https://research.ibm.com/blog/ibm-granite-guardian-5b-3b) - announcing Granite Guardian 3.2 5B and MoE 3B models
- **2025** - Paper accepted at **NAACL 2025**: "Evaluating the Prompt Steerability of Large Language Models"
- **2025** - Paper accepted at **NAACL 2025 Industry Track**: "Granite Guardian: Comprehensive LLM Safeguarding"
- **2025** - New preprint: "Agentic AI Needs a Systems Theory" - position paper on holistic approaches to agentic AI development

### 2024

- **December 2024** - Released [Granite Guardian](https://arxiv.org/abs/2412.07724) - a suite of safeguards for LLM risk detection achieving state-of-the-art results
- **December 2024** - Papers accepted at **NeurIPS 2024**:
  - "Are Uncertainty Quantification Capabilities of Evidential Deep Learning a Mirage?"
  - "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia"
  - "Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI"
- **November 2024** - Papers accepted at **EMNLP 2024**:
  - "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions"
  - "Value Alignment from Unstructured Text" (Industry Track)
- **July 2024** - Paper accepted at **ICML 2024**: "Thermometer: Towards Universal Calibration for Large Language Models"
- **June 2024** - Invited talk on LLM Governance and Alignment at the NAACL TrustNLP Workshop. [Slides](slides/naacl_trustnlp_talk.pdf)
- **2024** - Panel participation and talk on Reliable AI-assisted Decision Making at the National Academy of Sciences Decadal Survey

### 2023

- **December 2023** - Papers accepted at **NeurIPS 2023**:
  - "Efficient Equivariant Transfer Learning from Pretrained Models"
  - "Effective Human-AI Teams via Learned Natural Language Rules and Onboarding"
- **August 2023** - Invited talk on Uncertainty Calibration and AI-assisted Decision Making at the Workshop on Uncertainty Reasoning and Quantification in Decision Making, KDD
- **August 2023** - Panel participation and talk on Generative AI and Safety at the DSHealth Workshop, KDD
- **August 2023** - Panel participation on Trustworthy LLMs at the AI for Open Society Day, KDD
- **February 2023** - Papers accepted at **AAAI 2023** and **EACL 2023**:
  - "Post-hoc Uncertainty Learning Using a Dirichlet Meta-Model" (AAAI)
  - "Reliable Gradient-free and Likelihood-free Prompt Tuning" (EACL Findings)
